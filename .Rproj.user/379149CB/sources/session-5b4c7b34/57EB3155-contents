---
title: "Small results and Issue with DiceKriging.-- Should it be a problem?"
output:
  html_document: 
    code_download: true
  pdf_document: default
date: "2023-09-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
xfun::embed_file('results/maternal_healthreps100.txt')
library(tidyverse)
hn <- function(tab, filters = NULL){
    rename(tab, Var1=V1)%>%
    {if(!is.null(filters))
        filter(.,Var1 %in% filters)
      else .
    }%>%
    pivot_longer(!(Var1:V2), names_to = 'point',values_drop_na =TRUE,
                 names_transform = ~ 5* (parse_number(.)-3)) %>%
    summarise(mean = mean(1-value), .by = c(Var1, point))|>
    egoOptim::plotComparison(errorbars = FALSE)
}
```

## Results

I was able to run 200 replications for the maternal health dataset. (took 21 hours). Though the results are still not too stable.

```{r}
tb <- read.table("results/maternal_healthreps100.txt")
tb$V2 <- consecutive_id(tb$V2)
hn(tb)
```

After 150 iterations, it seems a bit stable. ie we can sample 150 iterations and plot them

```{r, fig.show="hold", out.width="30%"}
hn(tb[tb$V2 %in% sample(200, 150), ])
hn(tb[tb$V2 %in% sample(200, 150), ])
hn(tb[tb$V2 %in% sample(200, 150), ])
```

```{r, fig.show="hold", out.width="30%"}
hn(tb[tb$V2 %in% sample(200, 180), ])
hn(tb[tb$V2 %in% sample(200, 180), ])
hn(tb[tb$V2 %in% sample(200, 180), ])
```

We can note the idea of ourmethod improving quickly/faster that the other methods, as it rises above the other methods at the beginning denoting that it is suitable in scenarios with scarce resources.

Though in the long run, we cannot tell as to whether the method is significant than the rest in this particular dataset. I have embbeded the data within this html.

Trying to work on a dataset with fixed train and test data.

## An Issue with DiceKriging

While looking at the `km` function from `DiceKriging` package, it appears that at times the optimization of the lengthscale parameters does not converge. There is no warning or error produced. This instead leads to erroneous conclusions.

Here is a simple example:

Data

```{r}
km <- DiceKriging::km
X = data.frame(x1 = c(4.0,2,4.1,0.3,2), x2 = c(5.5,1.2,3.7,2,2.5))
y = c(4.2, 6.1, 0.2, 0.7, 5.2)
newX <- data.frame(x1 = 2, x2 = 2)
```

Model:

```{r}
set.seed(1)
model1 = km(design = X, response = y, covtype = 'gauss', control = list(trace = 0))
predict(model1, newX, type='UK')[c('mean', 'sd')]
```

Now suppose we run it with another seed?

```{r}
set.seed(3)
model2 = km(design = X, response = y, covtype = 'gauss', control = list(trace = 0))
predict(model2, newX, type='UK')[c('mean', 'sd')]
```

```{r}
set.seed(4)
model3 = km(design = X, response = y, covtype = 'gauss', control = list(trace = 0))
predict(model3, newX, type='UK')[c('mean', 'sd')]
```

```{r}
set.seed(7)
model4 = km(design = X, response = y, covtype = 'gauss', control = list(trace = 0))
predict(model4, newX, type='UK')[c('mean', 'sd')]
```

We note that the results are quite different. And this is just using a few seeds. Does the difference impact the results?

We first have to determine the plausible estimate for the new point $x_6 = (2,2)$

Looking at the data we see that the two close points are $x_5=(2.0,2.5)$ with $y_5=5.2$ and $x_2=(2.0,1.2)$ with $y_2 = 6.1$. Thus via intuition we can confidently state that the estimate for $(2.0,2.0)$ should be a number at least above 5. as only the second dimension changed. Yet in the above results we get values as small as 0.89. That difference it quite huge.

Doing it using `gstat` package:

```{r}
gstat::krige(y~1,~x1+x2, X, newX)
```

The estimate from the above with the defaults is around `5.053`. Notice that the standard error was not estimated. To estimate this, we would specify sill and range of the variogram.

```{r}
gstat::krige(y~1, ~x1+x2, X, newX, gstat::vgm(1,'Gau', 50))
```

I arbitrary chose range to be 50 and sill to be 1. Still we note that the estimate which is `5.326` is farther from the estimates given by DiceKriging.

Running in Python using default values.

```{python, warnings = FALSE}
import numpy as np
from skgstat import Variogram, OrdinaryKriging
import warnings
X = np.c_[[4.0,2,4.1,0.3,2], [5.5,1.2,3.7,2,2.5]]
y = np.r_[4.2, 6.1, 0.2, 0.7, 5.2]
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    V = Variogram(X,y, model='gaussian', normalize = False)
    ok = OrdinaryKriging(V, min_points = 1)
ok.transform(np.r_[[2]], np.r_[[2]])
```

```{python}
import matplotlib.pyplot as plt
from matplotlib.patches import Arrow, Circle
xx, yy = np.mgrid[0:5:100j, 0:6:100j].reshape(2,-1)
cx = np.c_[np.searchsorted(np.unique(xx), X[:,0]),np.searchsorted(np.unique(yy), X[:,1])]

field = ok.transform(xx.flatten(), yy.flatten()).reshape(100,100)
fig, ax = plt.subplots(1)
a = ax.imshow(field, origin='lower', cmap='plasma', vmin=V.values.min(), vmax=V.values.max())
for point in cx:
  ax.add_patch(Circle(point, radius=1, color='black'))
ax.add_patch(Circle((np.searchsorted(np.unique(xx), 2),np.searchsorted(np.unique(yy),2)),
radius=1,color='red'))
r = fig.colorbar(a)
ax.set_title('Kriging interpolation using gaussian kernel')
plt.show(fig)
```

We can note that the red point above (2,2) is clearly interpolated to be around/above 5.

I went ahead and replicated the results 1000 times, to see whether I can at least converge to a number greater than 5. But this was not the case.

```{r}
{model = km(design = X, response = y, covtype = 'gauss', control = list(trace = 0))
predict(model, newX, type='UK')$mean}|>
  replicate(n = 1000) -> a
mean(a)
```

A bootstrap to show the confidence Interval

```{r}
boot::norm.ci(boot::boot(a, \(x, i)mean(x[i]), 1000))
```

Notice how the 95% CI does not include the plausible estimate. This makes it tough to trust the DiceKriging results.

I went forth and implemented from scratch a kriging model using MLE to optimize the lengthscale parameters. Below is the code and the results.

```{r}
# Gaussian Kernel
gaus <- function(param,X1, X2){
  if(missing(X2)) X2 <- X1
  if(is.null(dim(X1))) X1 <- t(X1)
  if(is.null(dim(X2))) X2 <- t(X2)
  exp(-0.5*apply(X1, 1, \(v)colSums(((t(X2)-v)/param)^2)))
}

# Negative of the log likelihood to be minimixed.
negative_loglike <-function(param, X, y){ 
  if(any(param<=0)) return(NA)
  n = length(y)
  R = gaus(param, X)
  mu = sum(solve(R,y))/sum(solve(R, rep(1, n)))
  sig2 = drop((y - mu) %*% solve(R, y - mu)/n)
  n/2 * (log(2*pi*sig2) + log(det(R))/n + 1)
}

# obtain the optimal lengthscale parameters
X <- as.matrix(X)
newX <- as.matrix(newX)
param <- optim(colMeans(X), negative_loglike, X = X, y=y)$par

# Do the Prediction
my_krige <- function(param, X, y, newX){
  r <- gaus(param, X, newX)
  R <- gaus(param, X)
  Jn <- rep(1, n <- length(y))
  denom <- sum(solve(R, Jn))
  mu <- drop(sum(solve(R, y))/denom)
  sig2 = drop((y - mu) %*% solve(R, y - mu)/n)
  sig <- sig2*(1 - r %*%solve(R, r) +  (1 - Jn %*%solve(R, r))^2/denom)
  y_hat <- mu +  solve(R, r)%*%(y - mu)
  c(yhat=y_hat, sd=sqrt(sig), trend = mu, var = sig2)
}
my_krige(param, X, y, newX)
```

How can I confirm the code? Well if we use the range values from model 1 we should get the exact estimates as given in model1. Compare below:

```{r}
my_krige(model1@covariance@range.val, X, y, newX)
```

```{r}
c(unlist(predict(model1, newX, 'UK')[c("mean", "sd", "trend")]),
    var = model1@covariance@sd2)
```

Does the issue persist with other correlation functions? Yes. This is because in all the correlation functions used, there are parameters to be estimated using the MLE method. Thereby if the optimization does not converge, the problem is visible. eg. using the `matern` kernel:

```{r}
set.seed(3)
model = km(design = X, response = y, control = list(trace = 0))
predict(model, newX, type='UK')[c('mean', 'sd')]
```

```{r}
{model = km(design = X, response = y, control = list(trace = 0))
predict(model, newX, type='UK')$mean}|>
  replicate(n = 1000)|>mean()
```

While this is larger than using the Gaussian Kernel, It is still way below than the value that should be predicted.

Am curious as to whether the inconsistency with `DiceKriging::km` is also affecting the results. ie at times, the results for one method can be as low as half the other methods. Mostly it happens with our method and EGO, but our method is able to drastically rise in the next iteration to a result comparable to TREGO. Maybe considering using a different package for kriging.
